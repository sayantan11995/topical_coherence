{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "from __future__ imports must occur at the beginning of the file (<ipython-input-1-498ceb73beeb>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-498ceb73beeb>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    import sys\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m from __future__ imports must occur at the beginning of the file\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os, os.path\n",
    "import fnmatch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "import json\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "\n",
    "import pycountry\n",
    "import csv\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('words')\n",
    "with open('config.json') as handle:\n",
    "    config = json.load(handle)\n",
    "book = config['book_name']\n",
    "Parts = config['number_of_parts']\n",
    "dir_path = './%s/part'% book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_entities_map = {}\n",
    "entity_list_map = {}\n",
    "notation_int_map = {}\n",
    "location_list_map = {}\n",
    "# 0 = PERSON 1 = PLACE\n",
    "notation_int_map['PERSON'] = 0\n",
    "notation_int_map['GPE'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finding location list\n",
    "\n",
    "countries = {}\n",
    "city = {}\n",
    "for c in pycountry.countries:\n",
    "    countries[c.name] = 1\n",
    "with open ('city.csv','r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for raw in reader:\n",
    "        city[raw['city_name']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in range(1,Parts+1):\n",
    "    \n",
    "    chapter_entities_map[part] = {}\n",
    "    entity_list_map[part] = {}\n",
    "    location_list_map[part] = {}\n",
    "    x = 1\n",
    "    print(dir_path + str(part) + '/')\n",
    "    no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "    print('no.of_chapters:' + str(no_of_chapters))\n",
    "    print('part:' + str(part))\n",
    "    while x <= no_of_chapters:\n",
    "        \n",
    "        target_x = no_of_chapters+1\n",
    "        print('chapters' + str(x) + ' to ' + str(target_x) )\n",
    "        for i in range(x,target_x):\n",
    "            with open(dir_path + str(part) + '/chapter'+ str(i) + '.txt', 'r') as content_file:\n",
    "                content = content_file.read()\n",
    "            chapter_entities_map[part][i] = set()\n",
    "            location_list_map[part][i] = set()\n",
    "            content = unicode(content, \"utf-8\")\n",
    "            \n",
    "            for c in city:\n",
    "                if c in content:\n",
    "                    location_list_map[part][i].add(c)\n",
    "            for c in countries:\n",
    "                if c in content:\n",
    "                    location_list_map[part][i].add(c)\n",
    "                    \n",
    "            for sent in nltk.sent_tokenize(content):\n",
    "                for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "                    if not hasattr(chunk, 'label'):\n",
    "                        continue\n",
    "                    if chunk.label() not in notation_int_map.keys():\n",
    "                        continue\n",
    "\n",
    "                    entity = ' '.join(c[0] for c in chunk)\n",
    "                    # print(chunk.label(), entity)\n",
    "\n",
    "                    chapter_entities_map[part][i].add(entity)\n",
    "\n",
    "                    if entity not in entity_list_map.keys():\n",
    "                        entity_list_map[part][entity] = []  \n",
    "                    entity_list_map[part][entity].append(notation_int_map[chunk.label()])\n",
    "            chapter_entities_map[part][i] = list(chapter_entities_map[part][i])\n",
    "            location_list_map[part][i] = list(location_list_map[part][i])\n",
    "        #print chapter_entities_map\n",
    "        \n",
    "        x = target_x\n",
    "        \n",
    "with open('json_files/%s/nltk_chapter_entities_map.json' % book, 'w') as handle:\n",
    "    json.dump(chapter_entities_map,handle,indent=4)\n",
    "#print entity_list_map\n",
    "with open('json_files/%s/nltk_entity_list_map.json'% book, 'w') as handle:\n",
    "    json.dump(entity_list_map,handle,indent=4)\n",
    "with open('json_files/%s/location_list_map.json'% book, 'w') as handle:\n",
    "    json.dump(location_list_map,handle,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
