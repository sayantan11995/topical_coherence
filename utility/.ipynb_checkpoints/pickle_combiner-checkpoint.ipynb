{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3_new\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3_new\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3_new\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os, os.path\n",
    "import fnmatch\n",
    "import json\n",
    "import nltk\n",
    "import pycountry\n",
    "#nltk.download('wordnet')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "final_entity_list_map = {}\n",
    "entity_recognition_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as handle:\n",
    "    config = json.load(handle)\n",
    "book = config['book_name']\n",
    "Parts = config['number_of_parts']\n",
    "dir_path = './%s/part'% book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {}\n",
    "for i in list(pycountry.languages):\n",
    "    lang[i.name] = 1\n",
    "lang['Samskrit'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/%s/spacy_chapter_entities_map.json' % book) as handle:\n",
    "    spacy_chapter_entity_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/nltk_chapter_entities_map.json' % book) as handle:\n",
    "    nltk_chapter_entity_map = json.load(handle)\n",
    "    \n",
    "with open('json_files/%s/polygot_chapter_entities_map.json' % book) as handle:\n",
    "    polygot_chapter_entity_map = json.load(handle)\n",
    "                \n",
    "with open('json_files/%s/polygot_entity_list_map.json' % book) as handle:\n",
    "    polygot_entity_list_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/spacy_entity_list_map.json' % book) as handle:\n",
    "    spacy_entity_list_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/nltk_entity_list_map.json' % book) as handle:\n",
    "    nltk_entity_list_map = json.load(handle)\n",
    "            \n",
    "with open('json_files/%s/location_list_map.json' % book) as handle:\n",
    "    location_list_map = json.load(handle)\n",
    "    \n",
    "##comment/uncomment as final_person_list is ready    \n",
    "#with open('json_files/%s/final_person_list.json' % book) as handle:\n",
    "#   people_list_map = json.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Life_of_Mahatma_Gandhi/part1/\n",
      "no.of_chapters:15\n",
      "part:1\n",
      "chapters1 to 15\n",
      "./Life_of_Mahatma_Gandhi/part2/\n",
      "no.of_chapters:25\n",
      "part:2\n",
      "chapters1 to 25\n",
      "./Life_of_Mahatma_Gandhi/part3/\n",
      "no.of_chapters:16\n",
      "part:3\n",
      "chapters1 to 16\n"
     ]
    }
   ],
   "source": [
    "final_chapter_entity_map = {}\n",
    "wordnets = []\n",
    "        \n",
    "        \n",
    "for p in range(1,Parts+1):\n",
    "    part = str(p)\n",
    "    final_chapter_entity_map[part]={}\n",
    "    entity_recognition_map[part] = {}\n",
    "    for top_percent in [100]:\n",
    "        \n",
    "        x = 1\n",
    "        print(dir_path + str(part) + '/')\n",
    "        no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "        print('no.of_chapters:' + str(no_of_chapters))\n",
    "        print('part:' + str(part))\n",
    "        \n",
    "        print('chapters' + str(x) + ' to ' + str(no_of_chapters) )\n",
    "        total_entity_set = set()\n",
    "\n",
    "        for j in range(x,no_of_chapters+1):\n",
    "            i = str(j)\n",
    "            final_chapter_entity_map[part][i] = set(polygot_chapter_entity_map[part][i]) | set(spacy_chapter_entity_map[part][i]) | set(nltk_chapter_entity_map[part][i])\n",
    "            for e in final_chapter_entity_map[part][i]:\n",
    "                total_entity_set.add(e)\n",
    "            final_chapter_entity_map[part][i] = list(final_chapter_entity_map[part][i])\n",
    "\n",
    "            #entity should be pointed out by atleast 2 libraries\n",
    "            total_entity_set_copy = set()\n",
    "            for e in total_entity_set:\n",
    "                # removing entities which are languages\n",
    "                if e in lang:\n",
    "                    continue\n",
    "                presence_count = 0\n",
    "                if e in polygot_entity_list_map[part].keys():\n",
    "                    presence_count += 1\n",
    "                if e in spacy_entity_list_map[part].keys():\n",
    "                    presence_count += 1\n",
    "                if e in nltk_entity_list_map[part].keys():\n",
    "                    presence_count += 1\n",
    "\n",
    "                if presence_count < 2:\n",
    "                    continue\n",
    "                total_entity_set_copy.add(e)\n",
    "            total_entity_set = total_entity_set_copy\n",
    "\n",
    "        #checking if any entity is substring of another entity neglect if true\n",
    "            total_entity_c = total_entity_set\n",
    "            total_entity_set_cc = set()\n",
    "            for e in total_entity_set:\n",
    "                total_entity_c.remove(e)\n",
    "                if not any(e in s for s in total_entity_c):\n",
    "                    total_entity_set_cc.add(e)\n",
    "                total_entity_c.add(e)\n",
    "            total_entity_set = total_entity_set_cc\n",
    "\n",
    "            #print(total_entity_set)\n",
    "            \n",
    "            for e in total_entity_set:\n",
    "                \n",
    "                final_entity_list_map[e] = []\n",
    "                if e in polygot_entity_list_map[part].keys():\n",
    "                    final_entity_list_map[e].extend(polygot_entity_list_map[part][e])\n",
    "                if e in spacy_entity_list_map[part].keys():\n",
    "                    final_entity_list_map[e].extend(spacy_entity_list_map[part][e])\n",
    "                if e in nltk_entity_list_map[part].keys():\n",
    "                    final_entity_list_map[e].extend(nltk_entity_list_map[part][e])\n",
    "\n",
    "                entity_type = max(set(final_entity_list_map[e]), key=(final_entity_list_map[e]).count)\n",
    "                    \n",
    "\n",
    "                if entity_type==1 and e not in location_list_map[part][i]:\n",
    "                    #print(e,i)\n",
    "                    continue\n",
    "                if entity_type==0 and e in location_list_map[part][i]:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # person entity needs to be in people list map mind the cyclic dependancy due to manual filtering\n",
    "                #if entity_type == 0 and e not in people_list_map[part]:\n",
    "                #    continue\n",
    "                if wordnet.synsets(e) and entity_type==0:\n",
    "                    wordnets.append(e)\n",
    "                    continue\n",
    "                entity_recognition_map[part][e] = entity_type\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Life_of_Mahatma_Gandhi/part1/\n",
      "no.of_chapters:15\n",
      "part:1\n",
      "chapters1 to 15\n",
      "./Life_of_Mahatma_Gandhi/part2/\n",
      "no.of_chapters:25\n",
      "part:2\n",
      "chapters1 to 25\n",
      "./Life_of_Mahatma_Gandhi/part3/\n",
      "no.of_chapters:16\n",
      "part:3\n",
      "chapters1 to 16\n",
      "entity removed:0\n"
     ]
    }
   ],
   "source": [
    "#only keeping the entities that perfectly passed all filters in chapter entity map also\n",
    "for p in range(1,Parts+1):\n",
    "    part = str(p)\n",
    "    x = 1\n",
    "    print(dir_path + str(part) + '/')\n",
    "    no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "    print('no.of_chapters:' + str(no_of_chapters))\n",
    "    print('part:' + str(part))\n",
    "\n",
    "    print('chapters' + str(x) + ' to ' + str(no_of_chapters) )\n",
    "    entity_removed = 0\n",
    "    for j in range(x,no_of_chapters+1):\n",
    "        chapter_filtered_list = []\n",
    "        j = str(j)\n",
    "        for entity in final_chapter_entity_map[part][j]:\n",
    "            if entity in entity_recognition_map[part].keys():\n",
    "                chapter_filtered_list.append(entity)\n",
    "        final_chapter_entity_map[part][j] = chapter_filtered_list\n",
    "print(\"entity removed:\" + str(entity_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'Sir', u'Wylie', u'Khan', u'Christ', u'Governorships', u'Doctor', u'God', u'Indians', u'Malays', u'Esau', u'Jan', u'Ceylon', u'Edward', u'Drew', u'Krugers', u'Ellis', u'Moses', u'John', u'Chamberlain', u'Good', u'Wheat', u'Englishman', u'Cablegrams', u'Salt', u'Pathan', u'Hypocrisy', u'Hunter', u'Orientals', u'Lazarus', u'Jack', u'Arjuna', u'West', u'Roosevelt', u'Telugus', u'Lead', u'Smuts', u'Dick', u'Andrews', u'Philips', u'Hindustani', u'Morley', u'Bill', u'Boers', u'Koran', u'Socrates', u'Savage', u'Stead', u'Gandhi', u'Truth', u'Henry', u'Night', u'Curtis', u'Symonds', u'Gibson', u'Laughton', u'Jesus', u'Rand', u'White', u'Tolstoy', u'Love', u'Satyagraha', u'Damocles', u'Solitude', u'Distrust', u'Avesta', u'Hamlet', u'Kruger', u'Patience', u'Blacks', u'Joseph', u'Sleep', u'Booth', u'Granny', u'Phthisis', u'Polk', u'Mr', u'Islam'])\n"
     ]
    }
   ],
   "source": [
    "print set(wordnets)\n",
    "\n",
    "with open('json_files/%s/final_chapter_entity_map_tfidf.json' % book, 'w') as handle:\n",
    "    json.dump(final_chapter_entity_map,handle,indent = 4)\n",
    "\n",
    "with open('json_files/%s/entity_recognition_map_tfidf.json' % book, 'w') as handle:\n",
    "    json.dump(entity_recognition_map,handle,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/%s/entity_recognition_map_tfidf.json' % book) as handle:\n",
    "    entity_map = json.load(handle)\n",
    "place_list = {}\n",
    "person_list = {}\n",
    "for part in range(1,Parts+1):\n",
    "    part = str(part)\n",
    "    place_list[part] = set()\n",
    "    person_list[part] = set()\n",
    "    for entity in entity_map[part].keys():\n",
    "        if entity_map[part][entity] == 1:\n",
    "            place_list[part].add(entity)\n",
    "        else:\n",
    "            person_list[part].add(entity)\n",
    "    person_list[part] = list(person_list[part])\n",
    "    place_list[part] = list(place_list[part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only person list is needed to be filtered manually initial_person_list -> final_person_list\n",
    "with open('json_files/%s/initial_person_list.json' % book, 'w') as handle:\n",
    "    json.dump(person_list,handle,indent = 4)\n",
    "\n",
    "with open('json_files/%s/initial_place_list.json' % book, 'w') as handle:\n",
    "    json.dump(place_list,handle,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
