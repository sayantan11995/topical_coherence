{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateparser.search import search_dates\n",
    "import re\n",
    "import dateparser\n",
    "import os, os.path\n",
    "import fnmatch\n",
    "import datefinder\n",
    "from date_extractor import extract_dates\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as handle:\n",
    "    config = json.load(handle)\n",
    "book = config['book_name']\n",
    "Parts = config['number_of_parts']\n",
    "dir_path = './%s/part'% book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part:1\n",
      "./Lenin_Selected_Works/part1/\n",
      "no.of_chapters:27\n"
     ]
    }
   ],
   "source": [
    "currentYear = datetime.now().year\n",
    "date_dict = {}\n",
    "year_mode_dict = {}\n",
    "for part in range(1,Parts+1):\n",
    "    x = 1\n",
    "    print(\"part:\" + str(part))\n",
    "    date_dict[str(part)] = {}\n",
    "    year_mode_dict[str(part)] = {}\n",
    "    print(dir_path + str(part) + '/')\n",
    "    no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "    print('no.of_chapters:' + str(no_of_chapters))\n",
    "    while x <= no_of_chapters:\n",
    "        target_x = no_of_chapters+1\n",
    "        for i in range(x,target_x):\n",
    "            #print(\"chapter:\" + str(i))\n",
    "            with open(dir_path + str(part) + '/chapter'+ str(i) + '.txt', 'r', encoding='utf-8') as content_file:\n",
    "                content = content_file.read()\n",
    "            \n",
    "#             content = unicode(content, \"utf-8\")\n",
    "            #dates = search_dates(content)\n",
    "            #dates = extract_dates(content)\n",
    "            #dates = datefinder.find_dates(content)\n",
    "            \"\"\" # for \n",
    "            dates = []\n",
    "            matches = datefinder.find_dates(content)\n",
    "            for match in matches:\n",
    "                dates.append(match)\n",
    "                # print(type(match))\n",
    "            \"\"\"\n",
    "            regex= \"\\d{4}\"\n",
    "            dates = re.findall(regex, content)\n",
    "            mdates = []\n",
    "            year_list = []\n",
    "            try:\n",
    "                for date in dates:\n",
    "                    \"\"\"\n",
    "                    if int(date[1].year) < 1960 and int(date[1].year) > 1800:\n",
    "                    \n",
    "                        print date[1].year\n",
    "                        temp_date = {}\n",
    "                        temp_date['year'] = date[1].year\n",
    "                        temp_date['month'] = date[1].month\n",
    "                        temp_date['day'] = date[1].day\n",
    "                        year_list.append(date[1].year)\n",
    "                        mdates.append(temp_date)\n",
    "                        \"\"\"\n",
    "                    if int(date) < 1960 and int(date) > 1800:\n",
    "                    \n",
    "                        #print date\n",
    "                        year_list.append(date)\n",
    "                        mdates.append(date)\n",
    "                        \n",
    "            except:\n",
    "                pass\n",
    "            date_dict[str(part)][str(i)] = mdates\n",
    "            try:\n",
    "                year_mode_dict[str(part)][str(i)] = max(set(year_list), key=year_list.count)\n",
    "            except:\n",
    "                year_mode_dict[str(part)][str(i)] = -1\n",
    "        x = target_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#print date_dict\\nwith open('json_files/%s/date_dict.json' %book, 'w') as handle:\\n    json.dump(date_dict,handle,indent = 4)\\n\\nwith open('json_files/%s/year_mode_dict.json' %book, 'w') as handle:\\n    json.dump(year_mode_dict,handle,indent = 4)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#print date_dict\n",
    "with open('json_files/%s/date_dict.json' %book, 'w') as handle:\n",
    "    json.dump(date_dict,handle,indent = 4)\n",
    "\n",
    "with open('json_files/%s/year_mode_dict.json' %book, 'w') as handle:\n",
    "    json.dump(year_mode_dict,handle,indent = 4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    }
   ],
   "source": [
    "# assigning chapter the date with closest chapter\n",
    "def closest(lst, K):\n",
    "    return lst[min(range(len(lst)), key = lambda i: abs(lst[i]-K))]\n",
    "\n",
    "\"\"\"\n",
    "with open('json_files/%s/date_dict.json' %book) as handle:\n",
    "    date_dict= json.load(handle)\n",
    "\n",
    "with open('json_files/%s/year_mode_dict.json' %book) as handle:\n",
    "    year_mode_dict = json.load(handle)\n",
    "\"\"\"\n",
    "                 \n",
    "for part in range(1,Parts+1):\n",
    "    x = 1\n",
    "    # checking which chapters has date assigned\n",
    "    no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "    chapters_with_date = []\n",
    "    while x <= no_of_chapters:\n",
    "        target_x = no_of_chapters+1\n",
    "        for i in range(x,target_x):\n",
    "            # print(\"chapter:\" + str(i))\n",
    "            if year_mode_dict[str(part)][str(i)] != -1:\n",
    "                chapters_with_date.append(i)\n",
    "        x = target_x\n",
    "    x = 1\n",
    "    # assigning date to chapter which is closest to it\n",
    "    print(chapters_with_date)\n",
    "    while x <= no_of_chapters:\n",
    "        target_x = no_of_chapters+1\n",
    "        for i in range(x,target_x):\n",
    "            # print(\"chapter:\" + str(i))\n",
    "            k = closest(chapters_with_date,i)\n",
    "            #print(\"K:\",k)\n",
    "            year_mode_dict[str(part)][str(i)] = year_mode_dict[str(part)][str(k)]\n",
    "        x = target_x\n",
    "\n",
    "with open('json_files/%s/year_mode_dict_cassigned.json' %book, 'w') as handle:\n",
    "    json.dump(year_mode_dict,handle,indent = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('json_files/%s/date_entity_map.json' %book, 'w') as handle:\\n    json.dump(date_entity_map,handle,indent = 4)\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_entity_map = {} # every date will be mapped to\n",
    "with open('json_files/%s/entity_recognition_map_tfidf.json' %book) as handle:\n",
    "    entity_recognition_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/spacy_chapter_entities_map.json' % book) as handle:\n",
    "    spacy_chapter_entity_map = json.load(handle)\n",
    "\n",
    "\n",
    "with open('json_files/%s/spacy_chapter_entities_map.json' % book) as handle:\n",
    "    spacy_chapter_entity_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/nltk_chapter_entities_map.json' % book) as handle:\n",
    "    nltk_chapter_entity_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/polygot_chapter_entities_map.json' % book) as handle:\n",
    "    polygot_chapter_entity_map = json.load(handle)\n",
    "\n",
    "for p in range(1,Parts+1):\n",
    "    part = str(p)\n",
    "    date_entity_map[str(part)] = {}\n",
    "    x = 1\n",
    "    \n",
    "    # checking which chapters has date assigned\n",
    "    no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "    while x <= no_of_chapters:\n",
    "        target_x = no_of_chapters+1\n",
    "        \n",
    "        #entity recognition map has all the filtered entries\n",
    "        \n",
    "            \n",
    "        for i in range(x,target_x):\n",
    "            chapter_year = year_mode_dict[str(part)][str(i)]\n",
    "            if chapter_year not in date_entity_map[str(part)]:\n",
    "                date_entity_map[str(part)][str(chapter_year)] = set()\n",
    "            spacy_chapter_entity_map[part][i] = set(spacy_chapter_entity_map[part][str(i)])\n",
    "            nltk_chapter_entity_map[part][i] = set(nltk_chapter_entity_map[part][str(i)])\n",
    "            polygot_chapter_entity_map[part][i] = set(polygot_chapter_entity_map[part][str(i)])\n",
    "            final_chapter_entity_map = polygot_chapter_entity_map[part][i] | spacy_chapter_entity_map[part][i] | nltk_chapter_entity_map[part][i]\n",
    "            #print(\"final map:\")\n",
    "            #print final_chapter_entity_map\n",
    "            for e in final_chapter_entity_map:\n",
    "                if e in entity_recognition_map[part].keys():\n",
    "                    date_entity_map[str(part)][str(chapter_year)].add((e,entity_recognition_map[part][e]))\n",
    "        x = target_x\n",
    "        \n",
    "#converting all sets in date_entity_map to list\n",
    "for part in range(1,Parts+1):\n",
    "    for k in date_entity_map[str(part)].keys():\n",
    "        date_entity_map[str(part)][str(k)] = list(date_entity_map[str(part)][str(k)])\n",
    "\n",
    "\"\"\"\n",
    "with open('json_files/%s/date_entity_map.json' %book, 'w') as handle:\n",
    "    json.dump(date_entity_map,handle,indent = 4)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all entities of a particular year\n",
    "date_person_list = {} # independent of parts\n",
    "date_place_list = {}\n",
    "for part in range(1,Parts+1):\n",
    "    for k in date_entity_map[str(part)].keys():\n",
    "        k = str(k)\n",
    "        if k not in date_person_list.keys():\n",
    "            date_person_list[k] = set()\n",
    "        if k not in date_place_list.keys():\n",
    "            date_place_list[k] = set()\n",
    "        date_entity_map[str(part)][str(k)] = list(date_entity_map[str(part)][str(k)])\n",
    "        for element in date_entity_map[str(part)][str(k)]:\n",
    "            if element[1] == 1:\n",
    "                date_place_list[k].add(element[0])\n",
    "            else:\n",
    "                date_person_list[k].add(element[0])\n",
    "\n",
    "#converting all sets in date_entity_map to list\n",
    "for k in date_place_list.keys():\n",
    "    date_place_list[str(k)] = list(date_place_list[str(k)])\n",
    "\n",
    "for k in date_person_list.keys():\n",
    "    date_person_list[str(k)] = list(date_person_list[str(k)])\n",
    "        \n",
    "with open('json_files/%s/date_place_list.json' %book, 'w') as handle:\n",
    "    json.dump(date_place_list,handle,indent = 4)\n",
    "\n",
    "with open('json_files/%s/date_person_list.json' %book, 'w') as handle:\n",
    "    json.dump(date_person_list,handle,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reverse_dict(year_dict):\n",
    "    return_dict = {}\n",
    "    for k in year_dict.keys():\n",
    "        for entity in year_dict[k]:\n",
    "            if entity not in return_dict.keys():\n",
    "                return_dict[entity] = []\n",
    "            return_dict[entity].append(k)\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates an inverse dict i.e in which year are the particular entities occuring\n",
    "place_entity_year_dict = {}\n",
    "person_entity_year_dict = {}\n",
    "\n",
    "place_entity_year_dict = create_reverse_dict(date_place_list)\n",
    "person_entity_year_dict = create_reverse_dict(date_person_list)\n",
    "\n",
    "with open('json_files/%s/place_entity_year_dict.json' %book, 'w') as handle:\n",
    "    json.dump(place_entity_year_dict,handle,indent = 4)\n",
    "\n",
    "with open('json_files/%s/person_entity_year_dict.json' %book, 'w') as handle:\n",
    "    json.dump(person_entity_year_dict,handle,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79343538e6373866518bae8914e0fa7d73ef4b03ba66fe6bca0ffef2de4804d5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
