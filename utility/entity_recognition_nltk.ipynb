{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import os, os.path\n",
    "import fnmatch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "import json\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "\n",
    "import pycountry\n",
    "import csv\n",
    "import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('words')\n",
    "# nltk.download('all')\n",
    "with open('config.json') as handle:\n",
    "    config = json.load(handle)\n",
    "book = config['book_name']\n",
    "Parts = config['number_of_parts']\n",
    "dir_path = './%s/part'% book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_entities_map = {}\n",
    "entity_list_map = {}\n",
    "notation_int_map = {}\n",
    "location_list_map = {}\n",
    "# 0 = PERSON 1 = PLACE\n",
    "notation_int_map['PERSON'] = 0\n",
    "notation_int_map['GPE'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finding location list\n",
    "\n",
    "countries = {}\n",
    "city = {}\n",
    "for c in pycountry.countries:\n",
    "    countries[c.name] = 1\n",
    "with open ('city.csv','r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for raw in reader:\n",
    "        city[raw['city_name']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Lenin_Selected_Works/part1/\n",
      "no.of_chapters:27\n",
      "part:1\n",
      "chapters1 to 28\n"
     ]
    }
   ],
   "source": [
    "for part in range(1,Parts+1):\n",
    "    \n",
    "    chapter_entities_map[part] = {}\n",
    "    entity_list_map[part] = {}\n",
    "    location_list_map[part] = {}\n",
    "    x = 1\n",
    "    print(dir_path + str(part) + '/')\n",
    "    no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "    print('no.of_chapters:' + str(no_of_chapters))\n",
    "    print('part:' + str(part))\n",
    "    while x <= no_of_chapters:\n",
    "        \n",
    "        target_x = no_of_chapters+1\n",
    "        print('chapters' + str(x) + ' to ' + str(target_x) )\n",
    "        for i in range(x,target_x):\n",
    "            with open(dir_path + str(part) + '/chapter'+ str(i) + '.txt', 'r', encoding='utf-8') as content_file:\n",
    "                content = content_file.read()\n",
    "            chapter_entities_map[part][i] = set()\n",
    "            location_list_map[part][i] = set()\n",
    "#             content = unicode(content, \"utf-8\")\n",
    "            \n",
    "            for c in city:\n",
    "                if c in content:\n",
    "                    location_list_map[part][i].add(c)\n",
    "            for c in countries:\n",
    "                if c in content:\n",
    "                    location_list_map[part][i].add(c)\n",
    "                    \n",
    "                    \n",
    "            for sent in nltk.sent_tokenize(content):\n",
    "                for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "                    if not hasattr(chunk, 'label'):\n",
    "                        continue\n",
    "                    if chunk.label() not in notation_int_map.keys():\n",
    "                        continue\n",
    "\n",
    "                    entity = ' '.join(c[0] for c in chunk)\n",
    "                    # print(chunk.label(), entity)\n",
    "\n",
    "                    chapter_entities_map[part][i].add(entity)\n",
    "\n",
    "                    if entity not in entity_list_map.keys():\n",
    "                        entity_list_map[part][entity] = []  \n",
    "                    entity_list_map[part][entity].append(notation_int_map[chunk.label()])\n",
    "            chapter_entities_map[part][i] = list(chapter_entities_map[part][i])\n",
    "            location_list_map[part][i] = list(location_list_map[part][i])\n",
    "        #print chapter_entities_map\n",
    "        \n",
    "        x = target_x\n",
    "        \n",
    "with open('json_files/%s/nltk_chapter_entities_map.json' % book, 'w') as handle:\n",
    "    json.dump(chapter_entities_map,handle,indent=4)\n",
    "#print entity_list_map\n",
    "with open('json_files/%s/nltk_entity_list_map.json'% book, 'w') as handle:\n",
    "    json.dump(entity_list_map,handle,indent=4)\n",
    "with open('json_files/%s/location_list_map.json'% book, 'w') as handle:\n",
    "    json.dump(location_list_map,handle,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79343538e6373866518bae8914e0fa7d73ef4b03ba66fe6bca0ffef2de4804d5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
