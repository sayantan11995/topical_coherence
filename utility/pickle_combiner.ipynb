{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import fnmatch\n",
    "import json\n",
    "import nltk\n",
    "import pycountry\n",
    "#nltk.download('wordnet')\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "final_entity_list_map = {}\n",
    "entity_recognition_map = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json') as handle:\n",
    "    config = json.load(handle)\n",
    "book = config['book_name']\n",
    "Parts = config['number_of_parts']\n",
    "dir_path = './%s/part'% book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = {}\n",
    "for i in list(pycountry.languages):\n",
    "    lang[i.name] = 1\n",
    "lang['Samskrit'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/%s/spacy_chapter_entities_map.json' % book) as handle:\n",
    "    spacy_chapter_entity_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/nltk_chapter_entities_map.json' % book) as handle:\n",
    "    nltk_chapter_entity_map = json.load(handle)\n",
    "    \n",
    "with open('json_files/%s/polygot_chapter_entities_map.json' % book) as handle:\n",
    "    polygot_chapter_entity_map = json.load(handle)\n",
    "                \n",
    "with open('json_files/%s/polygot_entity_list_map.json' % book) as handle:\n",
    "    polygot_entity_list_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/spacy_entity_list_map.json' % book) as handle:\n",
    "    spacy_entity_list_map = json.load(handle)\n",
    "\n",
    "with open('json_files/%s/nltk_entity_list_map.json' % book) as handle:\n",
    "    nltk_entity_list_map = json.load(handle)\n",
    "            \n",
    "with open('json_files/%s/location_list_map.json' % book) as handle:\n",
    "    location_list_map = json.load(handle)\n",
    "    \n",
    "#comment/uncomment as final_person_list is ready    \n",
    "with open('json_files/%s/final_person_list.json' % book) as handle:\n",
    "    people_list_map = json.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Lenin_Selected_Works/part1/\n",
      "no.of_chapters:27\n",
      "part:1\n",
      "chapters1 to 27\n"
     ]
    }
   ],
   "source": [
    "final_chapter_entity_map = {}\n",
    "wordnets = []\n",
    "        \n",
    "        \n",
    "for p in range(1,Parts+1):\n",
    "    part = str(p)\n",
    "    final_chapter_entity_map[part]={}\n",
    "    entity_recognition_map[part] = {}\n",
    "    for top_percent in [100]:\n",
    "        \n",
    "        x = 1\n",
    "        print(dir_path + str(part) + '/')\n",
    "        no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "        print('no.of_chapters:' + str(no_of_chapters))\n",
    "        print('part:' + str(part))\n",
    "        \n",
    "        print('chapters' + str(x) + ' to ' + str(no_of_chapters) )\n",
    "        total_entity_set = set()\n",
    "\n",
    "        for j in range(x,no_of_chapters+1):\n",
    "            i = str(j)\n",
    "            final_chapter_entity_map[part][i] = set(polygot_chapter_entity_map[part][i]) | set(spacy_chapter_entity_map[part][i]) | set(nltk_chapter_entity_map[part][i])\n",
    "            for e in final_chapter_entity_map[part][i]:\n",
    "                total_entity_set.add(e)\n",
    "            final_chapter_entity_map[part][i] = list(final_chapter_entity_map[part][i])\n",
    "\n",
    "            #entity should be pointed out by atleast 2 libraries\n",
    "            total_entity_set_copy = set()\n",
    "            for e in total_entity_set:\n",
    "                # removing entities which are languages\n",
    "                if e in lang:\n",
    "                    continue\n",
    "                presence_count = 0\n",
    "                if e in polygot_entity_list_map[part].keys():\n",
    "                    presence_count += 1\n",
    "                if e in spacy_entity_list_map[part].keys():\n",
    "                    presence_count += 1\n",
    "                if e in nltk_entity_list_map[part].keys():\n",
    "                    presence_count += 1\n",
    "\n",
    "                if presence_count < 2:\n",
    "                    continue\n",
    "                total_entity_set_copy.add(e)\n",
    "            total_entity_set = total_entity_set_copy\n",
    "\n",
    "        #checking if any entity is substring of another entity neglect if true\n",
    "            total_entity_c = total_entity_set\n",
    "            total_entity_set_cc = set()\n",
    "            for e in total_entity_set:\n",
    "                total_entity_c.remove(e)\n",
    "                if not any(e in s for s in total_entity_c):\n",
    "                    total_entity_set_cc.add(e)\n",
    "                total_entity_c.add(e)\n",
    "            total_entity_set = total_entity_set_cc\n",
    "\n",
    "            #print(total_entity_set)\n",
    "            \n",
    "            for e in total_entity_set:\n",
    "                \n",
    "                final_entity_list_map[e] = []\n",
    "                if e in polygot_entity_list_map[part].keys():\n",
    "                    final_entity_list_map[e].extend(polygot_entity_list_map[part][e])\n",
    "                if e in spacy_entity_list_map[part].keys():\n",
    "                    final_entity_list_map[e].extend(spacy_entity_list_map[part][e])\n",
    "                if e in nltk_entity_list_map[part].keys():\n",
    "                    final_entity_list_map[e].extend(nltk_entity_list_map[part][e])\n",
    "\n",
    "                entity_type = max(set(final_entity_list_map[e]), key=(final_entity_list_map[e]).count)\n",
    "                    \n",
    "\n",
    "                if entity_type==1 and e not in location_list_map[part][i]:\n",
    "                    #print(e,i)\n",
    "                    continue\n",
    "                if entity_type==0 and e in location_list_map[part][i]:\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                # person entity needs to be in people list map mind the cyclic dependancy due to manual filtering\n",
    "                if entity_type == 0 and e not in people_list_map[part]:\n",
    "                    continue\n",
    "                if wordnet.synsets(e) and entity_type==0:\n",
    "                    wordnets.append(e)\n",
    "                    continue\n",
    "                entity_recognition_map[part][e] = entity_type\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Lenin_Selected_Works/part1/\n",
      "no.of_chapters:27\n",
      "part:1\n",
      "chapters1 to 27\n",
      "entity removed:0\n"
     ]
    }
   ],
   "source": [
    "#only keeping the entities that perfectly passed all filters in chapter entity map also\n",
    "for p in range(1,Parts+1):\n",
    "    part = str(p)\n",
    "    x = 1\n",
    "    print(dir_path + str(part) + '/')\n",
    "    no_of_chapters = len(fnmatch.filter(os.listdir(dir_path + str(part) + '/'), '*.txt'))\n",
    "    print('no.of_chapters:' + str(no_of_chapters))\n",
    "    print('part:' + str(part))\n",
    "\n",
    "    print('chapters' + str(x) + ' to ' + str(no_of_chapters) )\n",
    "    entity_removed = 0\n",
    "    for j in range(x,no_of_chapters+1):\n",
    "        chapter_filtered_list = []\n",
    "        j = str(j)\n",
    "        for entity in final_chapter_entity_map[part][j]:\n",
    "            if entity in entity_recognition_map[part].keys():\n",
    "                chapter_filtered_list.append(entity)\n",
    "        final_chapter_entity_map[part][j] = chapter_filtered_list\n",
    "print(\"entity removed:\" + str(entity_removed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(set(wordnets))\n",
    "\n",
    "with open('json_files/%s/final_chapter_entity_map_tfidf.json' % book, 'w') as handle:\n",
    "    json.dump(final_chapter_entity_map,handle,indent = 4)\n",
    "\n",
    "with open('json_files/%s/entity_recognition_map_tfidf.json' % book, 'w') as handle:\n",
    "    json.dump(entity_recognition_map,handle,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('json_files/%s/entity_recognition_map_tfidf.json' % book) as handle:\n",
    "    entity_map = json.load(handle)\n",
    "place_list = {}\n",
    "person_list = {}\n",
    "for part in range(1,Parts+1):\n",
    "    part = str(part)\n",
    "    place_list[part] = set()\n",
    "    person_list[part] = set()\n",
    "    for entity in entity_map[part].keys():\n",
    "        if entity_map[part][entity] == 1:\n",
    "            place_list[part].add(entity)\n",
    "        else:\n",
    "            person_list[part].add(entity)\n",
    "    person_list[part] = list(person_list[part])\n",
    "    place_list[part] = list(place_list[part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only person list is needed to be filtered manually initial_person_list -> final_person_list\n",
    "with open('json_files/%s/initial_person_list.json' % book, 'w') as handle:\n",
    "    json.dump(person_list,handle,indent = 4)\n",
    "\n",
    "with open('json_files/%s/initial_place_list.json' % book, 'w') as handle:\n",
    "    json.dump(place_list,handle,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79343538e6373866518bae8914e0fa7d73ef4b03ba66fe6bca0ffef2de4804d5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
